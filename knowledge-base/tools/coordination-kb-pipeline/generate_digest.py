#!/usr/bin/env python3
"""
Coordination KB Pipeline Digest Generator
Creates human-readable markdown reports from query logs

Part of: Coordination KB Pipeline (Recommendation #5)
"""

import json
from collections import Counter
from datetime import UTC, datetime
from pathlib import Path
from typing import Any


def load_query_log(
    log_file: Path, since: datetime | None = None
) -> list[dict[str, Any]]:
    """Load queries from JSONL log, optionally filtered by date"""
    queries: list[dict[str, Any]] = []
    if not log_file.exists():
        return queries

    with log_file.open() as f:
        for line in f:
            try:
                q = json.loads(line.strip())
                if since:
                    q_time = datetime.fromisoformat(q["timestamp"])
                    if q_time < since:
                        continue
                queries.append(q)
            except (json.JSONDecodeError, KeyError):
                pass

    return queries


def generate_digest(
    queries: list[dict[str, Any]], period: str = "all-time"
) -> str:
    """Generate markdown digest from queries"""
    if not queries:
        return (
            f"# Coordination KB Query Digest\n\n"
            f"**Period**: {period}\n\n_No queries in this period_\n"
        )

    # Temporal analysis
    timestamps = [datetime.fromisoformat(q["timestamp"]) for q in queries]
    first_query = min(timestamps)
    last_query = max(timestamps)

    # Unique queries and agents
    unique_queries = {q["query"] for q in queries}
    agents = Counter(q.get("origin_agent", "unknown") for q in queries)

    # Latency stats
    latencies = [q["query_latency_ms"] for q in queries if "query_latency_ms" in q]
    avg_latency = sum(latencies) / len(latencies) if latencies else 0
    max_latency = max(latencies) if latencies else 0

    # Top documents retrieved
    all_results = [
        result["doc_id"]
        for q in queries
        for result in q.get("top_results", [])
    ]
    top_docs = Counter(all_results).most_common(10)

    # Build digest
    digest = f"""# Coordination KB Query Digest

**Generated**: {datetime.now(UTC).strftime('%Y-%m-%d %H:%M:%S UTC')}
**Period**: {period}
**Date Range**: {first_query.strftime('%Y-%m-%d')} to {last_query.strftime('%Y-%m-%d')}

---

## ðŸ“Š Summary Statistics

| Metric | Value |
|--------|-------|
| Total Queries | {len(queries)} |
| Unique Queries | {len(unique_queries)} |
| Active Agents | {len(agents)} |
| Avg Latency | {avg_latency:.1f} ms |
| Max Latency | {max_latency} ms |

## ðŸ¤– Agent Activity

| Agent | Queries | Percentage |
|-------|---------|------------|
"""

    for agent, count in sorted(agents.items(), key=lambda x: -x[1]):
        pct = (count / len(queries)) * 100
        digest += f"| `{agent}` | {count} | {pct:.1f}% |\n"

    digest += """
## ðŸ“š Most Retrieved Documents

These documents were returned most frequently in top results:

| Rank | Document ID | Times Retrieved |
|------|-------------|-----------------|
"""

    for i, (doc_id, count) in enumerate(top_docs, 1):
        digest += f"| {i} | `{doc_id}` | {count} |\n"

    # Sample recent queries
    recent = sorted(queries, key=lambda q: q["timestamp"], reverse=True)[:10]

    digest += """
## ðŸ” Recent Queries

Most recent coordination queries:

"""

    for q in recent:
        timestamp = datetime.fromisoformat(q["timestamp"])
        agent = q.get("origin_agent", "unknown")
        query_text = q["query"]
        digest += (
            f"- **{timestamp.strftime('%Y-%m-%d %H:%M')}** [{agent}] \"{query_text}\"\n"
        )

    # Protocol version tracking
    versions = Counter(q.get("pipeline_version", "unknown") for q in queries)
    checksums = Counter(q.get("protocol_checksum", "unknown") for q in queries)

    digest += """
## ðŸ” Version Tracking

Pipeline versions and protocol checksums seen in this period:

**Pipeline Versions:**
"""

    for version, count in sorted(versions.items(), key=lambda x: -x[1]):
        digest += f"- `{version}`: {count} queries\n"

    digest += "\n**Protocol Checksums:**\n"

    for checksum, count in sorted(checksums.items(), key=lambda x: -x[1]):
        digest += f"- `{checksum}`: {count} queries\n"

    digest += """
---

_Generated by Coordination KB Pipeline Digest Generator_
_Part of: Multi-Agent Coordination System_
"""

    return digest


def main() -> None:
    import sys
    from datetime import timedelta

    log_file = Path(__file__).parent / "coordination_queries.jsonl"

    if not log_file.exists():
        print(f"No query log found at {log_file}")
        print("Run some queries first to generate digest data")
        sys.exit(1)

    # Parse arguments
    period = "all-time"
    since = None

    if len(sys.argv) > 1:
        period_arg = sys.argv[1]
        now = datetime.now(UTC)

        if period_arg == "--day":
            since = now - timedelta(days=1)
            period = "last-24-hours"
        elif period_arg == "--week":
            since = now - timedelta(days=7)
            period = "last-7-days"
        elif period_arg == "--month":
            since = now - timedelta(days=30)
            period = "last-30-days"
        elif period_arg.startswith("--since="):
            date_str = period_arg.split("=")[1]
            since = datetime.fromisoformat(date_str)
            period = f"since-{date_str}"

    # Load and process
    print(f"Loading queries from {log_file}...")
    queries = load_query_log(log_file, since=since)

    if not queries:
        print(f"No queries found for period: {period}")
        sys.exit(0)

    print(f"Generating digest for {len(queries)} queries...")
    digest = generate_digest(queries, period=period)

    # Save digest
    timestamp = datetime.now(UTC).strftime("%Y%m%d-%H%M%S")
    output_file = Path(__file__).parent / f"digest-{timestamp}.md"
    output_file.write_text(digest)

    print(f"\nâœ“ Digest saved to: {output_file}")
    print("\nPreview:\n")
    print(digest[:500] + "...\n")


if __name__ == "__main__":
    main()
